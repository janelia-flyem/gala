#!/usr/bin/env python

from ray import imio, agglo, morpho, classify, evaluate

try:
    from ray import stack_np
except ImportError:
    pass

import sys
import os
from skimage import morphology as skmorph
from scipy.ndimage import label
import argparse
import json
import h5py
import numpy
import logging
import textwrap

format_help = """\
Description of input JSON:
{
    #location of original grayscale
    "stack_path" : "<file_path/*.png>",

    #reuse prediction ilastik ilp for fast turnaround
    "ilastik_training_volume" : false,

    #volumes should use non zero boundary methodology
    "nonzero_boundary" : true,

    #use NeuroProof RAG and Stack for boosted performance
    "nonzero_boundary" : false,

    #efficient memory handling of large volumes
    "memory_mode" : false,
    
    #perform ilastik prediction
    "flow_generate_boundary_prediction" : false,

    #generate a watershed
    "flow_generate_watershed" : false,

    #median agglomeration
    "flow_perform_median_agglomeration" : false,

    #inclusion removal before agglomeration
    "flow_perform_inclusion_removal" : false,

    #inclusion removal after each agglomeration step
    "flow_perform_agglomeration_inclusion_removal" : false,

    #the thresholds where output is produced
    #there must be at least 0 there for some output
    "agglomeration_output_thresholds" : [ 0, 0.1 ],

    #ilp file produced by ilastik needed for boundary prediction
    "ilpfile" : "<ilastik ilp>",

    #manipulation of boundary map
    "boundary_manipulation_recipe" : [
        { "operation" : <type>, "params" : <params> },
        { "operation" : <type>, "params" : <params> },
        { "operation" : <type>, "params" : <params> }
    ],

    #connected component threshold
    "seed_cc_threshold" : 0,
    
    #only seeded watershed currently
    "seeded_watershed" : true,

    #threshold for picking seeds
    "watershed_seed_threshold" : 0,
    
    #manipulation of watershed seeds
    "seed_manipulation_recipe" : [
        { "operation" : <type>, "params" : <params> },
        { "operation" : <type>, "params" : <params> },
        { "operation" : <type>, "params" : <params> }
    ],

    #synapse-aware watershed and segmentation
    "synapse_file": "<file_path.json>"
}"""

FeatureHash = None
MasterLogger = None

# need to add custom functions
SeedActionFunction = None
BoundaryActionFunction = None

def init_features():
    global FeatureHash
    FeatureHash = {}
    FeatureHash["tiny"] = 0.3
    FeatureHash["small"] = 0.7
    FeatureHash["medium"] = 1.0
    FeatureHash["large"] = 1.6
    FeatureHash["huge"] = 3.5
    FeatureHash["megahuge"] = 5.0
    FeatureHash["gigahuge"] = 10.0

def init_actions():
    global SeedActionFunction
    global BoundaryActionFunction
    SeedActionFunction = {}
    BoundaryActionFunction = {}

class PipelineOptions:
    def __init__(self, initfile):
        json_file = open(initfile)
        self.json_val = json.load(json_file)
        json_file.close()
        json_save = json.dumps(self.json_val, indent=4)
        MasterLogger.debug(json_save)

        # load options
        self.stack_path = self.set_option("stack_path", required=True)
        self.ilastik_training_volume = self.set_option("ilastik_training_volume", False)
        self.nonzero_boundary = self.set_option("nonzero_boundary", True)
        self.use_neuroproof = self.set_option("use_neuroproof", False)
        self.memory_mode = self.set_option("memory_mode", False)
        self.flow_generate_boundary_prediction = self.set_option("flow_generate_boundary_prediction", False)
        self.flow_generate_watershed = self.set_option("flow_generate_watershed", False)
        self.flow_perform_median_agglomeration = self.set_option("flow_perform_median_agglomeration", False)
        self.flow_perform_inclusion_removal = self.set_option("flow_perform_inclusion_removal", False)
        self.flow_perform_agglomeration_inclusion_removal = self.set_option("flow_perform_agglomeration_inclusion_removal",
            default=False, warning = self.flow_perform_median_agglomeration)
        self.agglomeration_output_thresholds = self.set_option("agglomeration_output_thresholds", default = [])
        self.ilpfile = self.set_option("ilpfile", warning = False,
            required = self.flow_generate_boundary_prediction or self.ilastik_training_volume)
        self.features = self.set_option("features", default = [], warning = False)
        self.boundary_manipulation_recipe = self.set_option("boundary_manipulation_recipe", default = [],
            warning = self.flow_generate_watershed)

        self.seed_cc_threshold = self.set_option("seed_cc_threshold", default=0,
            warning = self.flow_generate_watershed)
        self.seeded_watershed = self.set_option("seeded_watershed", default=True,
            warning = self.flow_generate_watershed)
        self.watershed_seed_threshold = self.set_option("watershed_seed_threshold", default=0,
            warning = (self.flow_generate_watershed and self.seeded_watershed))
        self.seed_manipulation_recipe = self.set_option("seed_manipulation_recipe", default = [],
            warning = (self.flow_generate_watershed and self.seeded_watershed))
        self.synapse_file = \
            self.set_option("synapse_file", default=None, warning=False)

        if self.use_neuroproof and (self.synapse_file is not None):
            MasterLogger.warning("NeuroProof cannot be enabled with the options set")
            self.use_neuroproof = False

        if self.use_neuroproof:
            try:
                from ray import stack_np
            except ImportError:
                raise Exception("NeuroProof could not be imported")

    def set_option(self, json_name, default=None, warning=True, required=False):
        temp_option = self.json_val.get(json_name)
        if temp_option is None:
            if required:
                raise Exception(json_name + " needs to be specified")
            elif warning:
                MasterLogger.warning(json_name + " was not specified")
            temp_option = default
            MasterLogger.debug("JSON option set: " + json_name + " to default " + str(default))
        else:
            MasterLogger.debug("JSON option set: " + json_name)
        return temp_option


def set_logger(debug, log_filename, aux_filename = None):
    global MasterLogger
    MasterLogger = logging.getLogger('pipeline')



    MasterLogger.propagate = False
    MasterLogger.setLevel(logging.DEBUG)

    console = logging.StreamHandler(sys.stdout)
    if debug:
        console.setLevel(logging.DEBUG)
    else:
        console.setLevel(logging.INFO)

    formatter = logging.Formatter('%(levelname)-8s %(message)s')
    console.setFormatter(formatter)
    MasterLogger.addHandler(console)

    prim = logging.FileHandler(log_filename, 'a')
    prim.setLevel(logging.DEBUG)
    prim.setFormatter(logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s', datefmt='%m-%d-%y %H:%M'))
    MasterLogger.addHandler(prim)

    if aux_filename is not None:
        aux = logging.FileHandler(aux_filename, 'w')
        aux.setLevel(logging.DEBUG)
        aux.setFormatter(logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s', datefmt='%m-%d-%y %H:%M'))
        MasterLogger.addHandler(aux)

def flow_generate_boundary_prediction(options, image_stack, experimentprefix, output_dir):
    MasterLogger.info("Boundary Prediction Flow")
    
    ilastik_h5 = h5py.File(options.ilpfile, 'r')
    lset = ilastik_h5['DataSets/dataItem00/labels/data']
    lsetrest = lset[0,:,:,:]

    MasterLogger.info("Extracting labels")
    (dim1, dim2, dim3, dim4) = numpy.where(lsetrest > 0)
    num_labels = lsetrest.max()
    MasterLogger.info(str(num_labels) + " class labels")
    MasterLogger.info(str(len(dim1)) + " total label points extracted")

    for i in xrange(num_labels):
        (dim1, dum1, dum2, dum3) = numpy.where(lsetrest == (i+1))
        MasterLogger.info(str(len(dim1)) + " class " + str(i + 1) + " label points")
    featureset = ilastik_h5['Project/FeatureSelection/UserSelection']
    MasterLogger.info("Features selected...")
    for val in featureset:
        MasterLogger.info(str(val) + "\n")
    ilastik_h5.close()

    os.system(str("rm -f " + experimentprefix + ".h5"))
    imio.write_ilastik_batch_volume(image_stack, str(experimentprefix + ".h5"))

    #create temporary json for ilastik batch
    json_val_out = {}
    json_val_out["output_dir"] = output_dir
    json_val_out["session"] = options.ilpfile
    json_val_out["memory_mode"] = options.memory_mode
    json_val_out["images"] = []
    image_array = json_val_out["images"]
    image_array.append( { "name" : str(experimentprefix + ".h5") } )
    json_val_out["features"] = []

    #load features
    features = options.features
    for feat in iter(features):
        for featsize in features[feat]:
            json_val_out["features"].append([feat, FeatureHash[featsize] ])
    json_file_out = open(str(experimentprefix + ".json"), "w")
    json_out = json.dumps(json_val_out, indent=4)
    MasterLogger.debug("ilastik batch_fast json file: " + json_out)
    json_file_out.write(json_out)
    json_file_out.close()

    #run ilastik
    MasterLogger.info("Running Ilastik batch")
    ilastik_command = str("ilastik_batch_fast --config_file=" + experimentprefix + ".json")
    os.system(ilastik_command)

    #deleting ilastik
    os.system(str("rm -f " + experimentprefix + ".h5"))


def flow_generate_watershed(options, image_stack, experimentprefix, output_dir):
    MasterLogger.info("Watershed Flow")
    training_file = experimentprefix + ".h5_boundpred.h5"
    if not os.path.isfile(training_file):
        raise Exception("Training file not found: " + training_file)

    prediction0 = imio.read_image_stack(training_file, group='/volume/prediction')

    MasterLogger.info("Manipulating prediction")
    #prediction0 = prediction[...,0]
    if options.ilastik_training_volume:
        prediction0 = prediction0.transpose((2, 1, 0))
    for action in options.boundary_manipulation_recipe:
        if action["operation"] not in SeedActionFunction:
            MasterLogger.warning("boundary action " + action["operation"] + " was not found.  Skipping ...")
            continue
        MasterLogger.debug("performing boundary action " + action["operation"] + " with parameters: " + action["params"])
        BoundaryActionFunction[(action["operation"])](seeds, action["params"])

    MasterLogger.info("Manipulating seeds")
    # ?? is threshold a int or float
    MasterLogger.debug("watershed seed threshold: " + str(options.watershed_seed_threshold))
    seeds = label(prediction0<=options.watershed_seed_threshold)[0]

    if options.seed_cc_threshold > 0:
        MasterLogger.info("Removing small seeds")
        seeds = morpho.remove_small_connected_components(seeds, options.seed_cc_threshold)
        MasterLogger.info("Finished removing small seeds")

    for action in options.seed_manipulation_recipe:
        if action["operation"] not in SeedActionFunction:
            MasterLogger.warning("seed action " + action["operation"] + " was not found.  Skipping ...")
            continue
        MasterLogger.debug("performing seed action " + action["operation"] + " with parameters: " + action["params"])
        SeedActionFunction[(action["operation"])](seeds, action["params"])


    if options.synapse_file is not None:
        pre_post_pairs = syngeo.io.raveler_synapse_annotations_to_coords(
            args.synapse_file)
        synapse_volume = syngeo.io.volume_synapse_view(pre_post_pairs, p0.shape)
        seeds = morpho.minimum_seeds(seeds, numpy.nonzero(synapse_volume))
    MasterLogger.info("Starting watershed")
    watershed = skmorph.watershed(prediction0, seeds)
    MasterLogger.info("Finished watershed")
    return watershed, prediction0

def flow_perform_inclusion_removal(options, rag, watershed, prediction):
    if watershed is None:
        raise Exception("Watershed needed for inclusion removal")

    if rag is None:
        MasterLogger.info("Building RAG")
        if options.use_neuroproof:
            rag = stack_np.Stack(watershed, prediction) 
        else:
            rag = agglo.Rag(watershed, prediction, merge_priority_function=agglo.boundary_median,
                show_progress=True, nozeros=options.nonzero_boundary)
        MasterLogger.info("Finished building RAG")
    # ?? how to save rag
    MasterLogger.info("Starting inclusion removal with " + str(rag.number_of_nodes()) + " nodes")
    rag.remove_inclusions()
    MasterLogger.info("Finished inclusion removal with " + str(rag.number_of_nodes()) + " nodes")

    return rag


def flow_perform_median_agglomeration(options, rag, watershed, prediction, image_stack, experiment_name, output_dir, provenance_filename):
    if watershed is None:
        raise Exception("Watershed needed for median agglomeration")

    if rag is None:
        MasterLogger.info("Building RAG")
        if options.use_neuroproof:
            rag = stack_np.Stack(watershed, prediction) 
        else:
            rag = agglo.Rag(watershed, prediction, merge_priority_function=agglo.boundary_median,
                show_progress=True, nozeros=options.nonzero_boundary)
        MasterLogger.info("Finished building RAG")
    # ?? how to save rag
    if options.synapse_file is not None:
        pre_post_pairs = syngeo.io.raveler_synapse_annotations_to_coords(
            args.synapse_file)
        synapse_volume = \
            syngeo.io.volume_synapse_view(pre_post_pairs, watershed.shape)
        rag.set_exclusions(synapse_volume)
    sorted_output_thresholds = sorted(options.agglomeration_output_thresholds)
    sps_out = None
    for threshold in sorted_output_thresholds:
        MasterLogger.info("Starting agglomeration to threshold " + str(threshold) + " with " + str(rag.number_of_nodes()) + " nodes")
        rag.agglomerate(threshold)
        MasterLogger.info("Finished agglomeration to threshold " + str(threshold) + " with " + str(rag.number_of_nodes()) + " nodes")
        removed_inclusions = False
        if options.flow_perform_agglomeration_inclusion_removal and (threshold > 0 or not options.flow_perform_inclusion_removal):
            removed_inclusions = True
            MasterLogger.info("Starting agglomeration inclusion removal at threshold " + str(threshold) + " with " + str(rag.number_of_nodes()) + " nodes")
            rag.remove_inclusions()
            MasterLogger.info("Finished agglomeration inclusion removal at threshold " + str(threshold) + " with " + str(rag.number_of_nodes()) + " nodes")

        sps_out = output_raveler(rag, watershed, image_stack, experiment_name,
                    output_dir, provenance_filename, threshold, sps_out)

    return rag

def output_raveler(rag, watershed, image_stack, experiment_name, output_dir, 
        provenance_filename, threshold, sps_out = None):
    outdir = output_dir + "/raveler-export/" + experiment_name + "-median-" + str(threshold) + "/"
    MasterLogger.info("Exporting segmentation to Raveler " + outdir)
    provenance_raveler_file = outdir + "segmentation_pipeline.log"

    seg = None

    if watershed is None:
        raise Exception("No watershed created")

    if rag is None:
        seg = watershed
    else:
        seg = rag.get_segmentation()

    rav = imio.segs_to_raveler(watershed, seg, 0, do_conn_comp=False, sps_out=sps_out)
    sps_out, dummy1, dummy2 = rav
    os.system(str("rm -rf " + outdir))
    imio.write_to_raveler(*rav, directory=outdir, gray=image_stack)
    os.system("cp " + provenance_filename + " " + provenance_raveler_file)
    return sps_out


def main(argv):
    parser = argparse.ArgumentParser(description="Segmentation pipeline (featuring boundary prediction, median agglomeration, and inclusion removal)", formatter_class=argparse.RawDescriptionHelpFormatter, epilog=format_help)
    parser.add_argument('--initfile', type=str, help="Initial configuration json file needed",
        dest='initfile', required=True)
    parser.add_argument('--experiment-name', type=str, help="Name of the experiment -- will be used as the base name for all outputs", dest='experiment_name', required=True)
    parser.add_argument('--output-dir', type=str, help="Directory for all output", default='./', dest='output_dir')
    parser.add_argument('--debug', action="store_true", help="Show debug output", dest="debug", default=False)
    args = parser.parse_args()

    experiment_name = args.experiment_name
    output_dir = args.output_dir
    experimentprefix = output_dir + "/" + experiment_name

    provenance_filename = False

    try:
        # set logger to console and set log file logcation
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        log_filename = experimentprefix + ".segmentation_pipeline.log"
        provenance_filename = output_dir + "/" + experiment_name + ".provenance.log"
        set_logger(args.debug, log_filename, aux_filename = provenance_filename)
        MasterLogger.info("Script called: " + ' '.join(sys.argv))
        MasterLogger.info("Script run from: " + os.path.realpath('.'))
        MasterLogger.info("Experimental path and name: " + experimentprefix)

        # parse json and set global options
        options = PipelineOptions(args.initfile)

        init_features()
        # !! add relevant actions for seed and boundary manipulation
        init_actions()

        image_stack = imio.read_image_stack(options.stack_path)

        if options.flow_generate_boundary_prediction and not options.ilastik_training_volume:
            flow_generate_boundary_prediction(options, image_stack, experimentprefix, output_dir)
        elif options.ilastik_training_volume:
            MasterLogger.info("Loading saved ilastik volume")
            os.system("rm -f " + str(experimentprefix + ".h5_boundpred.h5"))
            imio.write_ilastik_batch_volume(image_stack, str(experimentprefix + ".h5_boundpred.h5"))
            image_stack = image_stack.transpose((0, 2, 1))
            f1 = h5py.File(str(experimentprefix + ".h5_boundpred.h5"), 'a')
            f2 = h5py.File(options.ilpfile, 'r')
            f1.copy(f2['/DataSets/dataItem00/prediction'], 'volume/prediction')
            f1.close()
            f2.close()

        watershed = None
        prediction = None
        rag = None

        if options.flow_generate_watershed:
            watershed, prediction = flow_generate_watershed(options, image_stack, experimentprefix, output_dir)

        if options.flow_perform_inclusion_removal:
            # ?? how to save previous watershed to avoid recomputation
            rag = flow_perform_inclusion_removal(options, rag, watershed, prediction)

        if options.flow_perform_median_agglomeration:
            rag = flow_perform_median_agglomeration(options, rag, watershed, prediction, image_stack,
                experiment_name, output_dir, provenance_filename)

        else:
            if 0 in options.agglomeration_output_thresholds:
                output_raveler(rag, watershed, image_stack, experiment_name, output_dir, provenance_filename, 0)

        os.system(str("rm -f " + provenance_filename))

    except Exception, e:
        if MasterLogger is not None:
            MasterLogger.exception(e)
        os.system(str("rm -f " + provenance_filename))
    except KeyboardInterrupt, err:
        if MasterLogger is not None:
            MasterLogger.exception(err)
        os.system(str("rm -f " + provenance_filename))

sys.exit(main(sys.argv))
